#+title: Adventures in GHC build times
#+author: doyougnu
#+date: <2021-07-09 Fri>

* What
  This page serves as a central repository for the effort to track and improve
  GHC compiler performance, where performance in this context specifically means
  build times. I (Jeff) will try to track ideas, issues, and relevant merge
  requests for future work as well.

* Ideas

** IntMap becomes unbalanced

*** Hypothesis
    ~IntMap~ is used throughout the compiler, typically storing ~Unique~ for
    keys. The idea here is that the ~IntMap~ is becoming very unbalanced. While
    this isn't a problem in and of itself because under the hood ~IntMap~'s are
    Patricia trie in specific cases if the keys share a long prefix of bits
    then the spine of the tree needs to be rebuilt for every insertion, thus
    leading to performance degradation. See [[https://gitlab.haskell.org/ghc/ghc/-/issues/19820#note_351497][this comment]] for a more precise
    discussion.

*** Status
    Unconfirmed. In progress, working on (2) in courses of action

*** Evidence
    To gather evidence that the unbalancing is happening we need to either
    inspect the heap or print the trees during a build. Any other ideas
    appreciated.

*** The Fix
    The current ~Unique~ implementation is big endian, i.e., it stores the "key"
    in the most significant bits of an ~Int~. There are several paths forward:

    1. hash the keys to make the probability of a long prefix more unlikely
       thereby increasing
    2. move the keys to little endian to observe a difference

*** Relevant Issues
     - https://gitlab.haskell.org/ghc/ghc/-/issues/19820
     - https://gitlab.haskell.org/ghc/ghc/-/issues/18541#note_292432 see Sylvain
       Henry's comment on ~IntMap.lookup~
     - https://github.com/haskell/containers/pull/340#issuecomment-610400875

*** Relevant Merge Requests
    (1) has been tried in [[https://gitlab.haskell.org/ghc/ghc/-/merge_requests/5068][!5068]] by [[https://gitlab.haskell.org/sgraf812][@sgraf812]], who implemented the key hash but
    didn't fix compilation errors

*** Courses of Action
    1. Retrieve direct evidence of the tree becoming unbalanced
    2. Revive [[https://gitlab.haskell.org/ghc/ghc/-/merge_requests/5068][!5068]], fix the compilation errors and benchmark
    3. Create a patch with [[https://github.com/haskell/containers/pull/340][novel IntMap implementation]] and benchmark
    4. Use a mutable map for the bits of the compiler which are already in IO.
       This hasn't been tried but would be a significant change as we would have
       to add the ~hashtables~ package as a boot or core library (not sure
       which). Although [[https://github.com/haskell-perf/dictionaries][benchmarks by Chris Done]] indicate a 1000x speedup over
       pure IntMap's.
    5. Ed Kmett has also experimented with Clojure-style RRB trees in his [[https://github.com/ekmett/transients][here]].
       The implementation is unfinished and there are no benchmarks. We could
       finish the implementation, benchmark it against the standard ~IntMap~, if
       it looks good then patch it into the compiler and benchmark.

*** What was learned
    - ~Not worth the cost~: From (3) in Courses of Action we saw that the
      ~bounded-intmap~ implementation as a drop in replacement for the current
      intmaps results in significant slowdowns (as much as 70%) in certain
      phases for certain packages. Furthermore the max speedup observed was 12%.
      Furthermore the nofib results only showed degradation's in compile time
      performance. See my analysis [[https://gitlab.haskell.org/ghc/ghc/-/issues/19820#note_364086][here]] for more details and data.


** IntMap ~lookup~ performs allocation

*** Hypothesis
    IntMap lookup performs allocation due to the key being specialized in its
    definition. See SPJ's breakdown [[https://gitlab.haskell.org/ghc/ghc/-/issues/20069][here]].

*** Status
    Confirmed without direct evidence.

*** Evidence
    - Note taken on [2021-07-29 Thu 09:09] \\
      Some new data. Consider this ticky I pulled off a profiled ~ghcHEAD~ sorted by
      allocations again:
      #+begin_src shell
         4177975  500907320          0   4 MEiM                 $waboveNest{v r4Wi} (GHC.Utils.Ppr) (fun)
          4379509  398865488          0   3 MEM                  beside{v rUb} (GHC.Utils.Ppr) (fun)
           227489   69156656          0   2 SS                   GHC.IO.Encoding.UTF8.mkUTF1{v r2sH} (fun)
           824290   50234672          0   3 i.M                  Data.IntMap.Internal.$winsert{v rg6b} (fun)
           160709   50141208    5142688   7 >.pMpME              $wact1{v s5rN} (GHC.IO.Handle.Text) (fun) in r5ja
           557080   48582464          0   2 >L                   GHC.Base.map{v 01X} (fun)
           414037   47448864          0   4 iiiM                 $l$s$wget1_g5mW{v} (GHC.Utils.Ppr) (fun)
           319828   30668960    3155760   1 i                    unpack{v s3Qc} (GHC.Utils.Encoding) (fun) in r3Bo
           241869   29278576          0   4 EMiL                 GHC.Utils.Ppr.$wsep1{v r4UI} (fun)
           313030   23752168          0   1 M                    oneLiner{v rUv} (GHC.Utils.Ppr) (fun)
           356156   22793984          0   4 LM>p                 GHC.IO.Handle.Internals.$wdo_operation{v r4SA} (fun)
           355989   19935384          0   4 LMp>                 GHC.IO.Handle.Internals.$wwantWritableHandle'{v r4SK} (fun)
      #+end_src

      Notice the entries (first column) and allocations (second column) for the lazy
      ~insert{v rg6b}~. I added ~hashable~ as a boot library and then hashed ~Uniques~ as they
      were created, like this:
      #+begin_src haskell
        mkUnique :: Char -> Int -> Unique
                                                -- Builds a unique from pieces
        -- EXPORTED and used only in GHC.Builtin.Uniques
        mkUnique c i
          = MkUnique (tag .|. bits)
          where
            tag  = ord c `shiftL` uNIQUE_BITS
            bits = (hash i) .&. uniqueMask
      #+end_src

      Now consider the resulting ticky profile:
      #+begin_src shell
         110499    4759232          0   3 i.M                  Data.IntMap.Internal.$winsert{v reYM} (fun)
            74036    3785808          0   4 ..MM                 Data.Map.Internal.balanceR{v r2Zo} (fun)
            31668    1645920          0   4 ..MM                 Data.Map.Internal.balanceL{v r2Zn} (fun)
            31084    1407840          0   2 LL                   GHC.Base.++{v 03} (fun)
             4852    1314304        320   2 LS                   go45{v sxOt} (GHC.IfaceToCore) (fun) in rowA
            40325    1299344          0   8 >MiipS.M             GHC.Unit.Module.Env.$w$sgo9{v r4iJ} (fun)
             5816    1179008       4608   1 L                    go26{v sxGg} (GHC.Unit.State) (fun) in sxGa
              106    1112440       2544   1 S                    $wgo{v s3Ns} (GHC.Foreign) (fun) in s3Q9
               20    1079704       1280   1 S                    sat_sfDb{v} (GHC.Iface.Binary) (fun,se) in rbYU
             8060     967200          0   3 pIS                  GHC.Data.FastString.$wmkNewFastStringShortByteString{v r7Qv} (fun)
            10875     878592          0   2 >L                   GHC.Base.map{v 01X} (fun)
            10781     773760          0   2 >p                   GHC.Data.FastString.$wmkFastStringWith{v r7Qw} (fun)
             5233     752976          0   8 SiiipMii             $walexGetByte{v rn73} (GHC.Parser.Lexer) (fun)
             4832     741096          0   4 Sppp                 GHC.Iface.Syntax.$w$cget1{v rh2S} (fun)
             2826     678240          0   1 >                    lexStrItem{v r3aM} (Text.Read.Lex) (fun)
            78656     553344          0   7 >iipS.M              GHC.Unit.State.$w$sgo9{v rkg5} (fun)
             5770     500896          0   4 Sppp                 GHC.Types.Name.Occurrence.$w$cget{v r8aZ} (fun)
            16769     460544          0   2 MM                   Text.ParserCombinators.ReadP.$fAlternativeP_$c<|>{v r1V5} (fun)
             1274     458640          0   1 M                    GHC.Types.Id.Make.mkPrimOpId{v r3} (fun)
      #+end_src

      Its completely changed, but more importantly the ~balance~ functions have risen
      to the top, entries are much smaller and ~insert~ has a 90% reduction in
      allocations and an 86% reduction in calls. This makes sense, under the
      hypothesis that the ~IntMap~s are becoming very unbalanced, then insert will
      have /more/ recursive calls but if the Uniques are not jumping in the most
      significant bits (as hashing will force) then we should observe /less/ recursion
      and thus less allocations.

    Noticed in [[https://gitlab.haskell.org/ghc/ghc/-/issues/18541#note_292432][this comment]], however not
    confirmed with direct evidence. See Sebastian's [[https://gitlab.haskell.org/ghc/ghc/-/issues/20069#note_362952][comment]] about the ~go~
    closure. There is also the discrepency provided by the ~weigh~ library. I
    just modified the ~intmap-benchmark~ target from containers, you'll see that
    ~findWithDefault~ has better allocation performance after a certain
    threshold due to the GC. Here's the code.
     #+begin_src haskell
     lookup :: [Int] -> M.IntMap Int -> Int
     lookup xs m = foldl' (\n k -> (fromMaybe n (M.lookup k m))) 0 xs

     lookupWithDefault :: [Int] -> Int -> M.IntMap Int -> Int
     lookupWithDefault xs d m = foldl' (\n k -> (M.findWithDefault k n m)) d xs

       main = do
         let m = M.fromAscList elems :: M.IntMap Int
         evaluate $ rnf [m]
         mainWith $ do
           func "lookup 8" (lookup (take (2 ^ 8) keys)) m
           func "lookupWithDefault 8" (lookupWithDefault (take (2 ^ 8) keys) 0) m
           func "lookup 9" (lookup (take (2 ^ 9) keys)) m
           func "lookupWithDefault 9" (lookupWithDefault (take (2 ^ 9) keys) 0) m
           func "lookupWithDefault 10" (lookupWithDefault (take (2 ^ 10) keys) 0) m
           func "lookup 10" (lookup (take (2 ^ 10) keys)) m
           func "lookupWithDefault 11" (lookupWithDefault (take (2 ^ 11) keys) 0) m
           func "lookup 11" (lookup (take (2 ^ 11) keys)) m
           func "lookupWithDefault 12" (lookupWithDefault (take (2 ^ 12) keys) 0) m
           func "lookup 12" (lookup (take (2 ^ 12) keys)) m
           func "lookupWithDefault 13" (lookupWithDefault (take (2 ^ 13) keys) 0) m
           func "lookup 13" (lookup (take (2 ^ 13) keys)) m
           func "lookupWithDefault 14" (lookupWithDefault (take (2 ^ 14) keys) 0) m
           func "lookup 14" (lookup (take (2 ^ 14) keys)) m
           func "lookupWithDefault 15" (lookupWithDefault (take (2 ^ 15) keys) 0) m
           func "lookup 15" (lookup (take (2 ^ 15) keys)) m
           func "lookupWithDefault 16" (lookupWithDefault (take (2 ^ 16) keys) 0) m
           func "lookup 16" (lookup (take (2 ^ 16) keys)) m
           func "lookupWithDefault 17" (lookupWithDefault (take (2 ^ 17) keys) 0) m
           func "lookup 17" (lookup (take (2 ^ 17) keys)) m
           func "lookupWithDefault 18" (lookupWithDefault (take (2 ^ 18) keys) 0) m
           func "lookup 18" (lookup (take (2 ^ 18) keys)) m
           func "lookupWithDefault 19" (lookupWithDefault (take (2 ^ 19) keys) 0) m
           func "lookup 19" (lookup (take (2 ^ 19) keys)) m
           func "lookupWithDefault 20" (lookupWithDefault (take (2 ^ 20) keys) 0) m
           func "lookup 20" (lookup (take (2 ^ 20) keys)) m
         where
           elems = zip keys values
           keys = [1 .. 2 ^ 22]
           values = [1 .. 2 ^ 22]
           sum k v1 v2 = k + v1 + v2
           consPair k v xs = (k, v) : xs
     #+end_src

     and the results:
      #+begin_src shell
      Running 1 benchmarks...
      Benchmark intmap-benchmarks: RUNNING...

      Case                    Allocated  GCs
      lookup 8                   39,224    0
      lookupWithDefault 8        33,080    0
      lookup 9                   78,136    0
      lookupWithDefault 9        65,848    0
      lookupWithDefault 10      131,384    0
      lookup 10                 155,960    0
      lookupWithDefault 11      262,456    0
      lookup 11                 311,608    0
      lookupWithDefault 12      524,600    0
      lookup 12                 622,904    0
      lookupWithDefault 13    1,048,888    1
      lookup 13               1,245,496    1
      lookupWithDefault 14    2,097,464    2
      lookup 14               2,490,680    2
      lookupWithDefault 15    4,194,616    4
      lookup 15               4,981,048    4
      lookupWithDefault 16    8,388,984    8
      lookup 16               9,961,848    9
      lookupWithDefault 17   16,777,592   16
      lookup 17              19,923,320   19
      lookupWithDefault 18   33,554,808   32
      lookup 18              39,846,264   38
      lookupWithDefault 19   67,109,240   64
      lookup 19              79,692,152   76
      lookupWithDefault 20  134,218,104  128
      lookup 20             159,383,928  153
         2,228,228,536 bytes allocated in the heap
         5,022,014,208 bytes copied during GC
           812,634,240 bytes maximum residency (16 sample(s))
             6,487,936 bytes maximum slop
                  1588 MiB total memory in use (0 MB lost due to fragmentation)

                                           Tot time (elapsed)  Avg pause  Max pause
        Gen  0      2138 colls,     0 par    9.619s   9.621s     0.0045s    0.0097s
        Gen  1        16 colls,     0 par   11.739s  11.739s     0.7337s    2.6470s

        INIT    time    0.001s  (  0.001s elapsed)
        MUT     time    3.036s  (538.025s elapsed)
        GC      time   17.382s  ( 17.384s elapsed)
        RP      time    0.000s  (  0.000s elapsed)
        PROF    time    3.976s  (  3.976s elapsed)
        EXIT    time    0.000s  (  0.000s elapsed)
        Total   time   24.394s  (559.386s elapsed)

        %GC     time       0.0%  (0.0% elapsed)

        Alloc rate    734,009,009 bytes per MUT second

        Productivity  28.7% of total user, 96.9% of total elapsed

      Benchmark intmap-benchmarks: FINISH
      #+end_src
      That's a 15.8% difference in allocations for 2^20 lookups and 25 more GCs!
      Judging from the allocations in the ticky profiles floating around
      (usually around 55,354,240) I bet GHC is in the between 2^18 and 2^19
      lookups. That means we should observe a speedup ~15%. What's surprising is
      that /every/ difference in the allocs above is ~15%. Or in other words the
      gap between the two remains the same as the number of lookups increases.


*** The Fix
    Sylvain Henry has a patch [[https://gitlab.haskell.org/ghc/ghc/-/issues/18541#note_292432][here]] but only tested the intmap-benchmarks.

*** Relevant Issues
     - https://gitlab.haskell.org/ghc/ghc/-/issues/19820 The low-hanging fruit
       issue kicked off by Richard Eisenberg's ticky ticky profile.
     - https://gitlab.haskell.org/ghc/ghc/-/issues/18541#note_292432 see Sylvain
       Henry's comment on ~IntMap.lookup~
     - https://gitlab.haskell.org/ghc/ghc/-/issues/20069 SPJ's IntMap issue

*** Relevant Merge Requests

*** Relevant Patches
    - see https://gitlab.haskell.org/ghc/ghc/-/issues/18541#note_292432

*** Courses of Action
    - Implement and benchmark Sylvain Henry's patch, benchmark it for building
      entire packages not just the intmap-benchmark

    - I took a deep dive into the core, stg of the lookup. First thing to notice
      is the core:
       #+begin_src haskell
       $wlookup
         = \ @a_s5OwJ ww_s5OwO w_s5OwL ->
             join {
               exit_X9 dt_d5E7p x_a5sqR
                 = case ==# ww_s5OwO dt_d5E7p of {
                     __DEFAULT -> Nothing;
                     1# -> Just x_a5sqR
                   } } in
             joinrec {
               go3_s5GTZ ds_d5zfo
                 = case ds_d5zfo of {
                     Bin dt_d5E7n dt1_d5E7o l_a5sqO r_a5sqP ->
                       let { m_s5GU1 = int2Word# dt1_d5E7o } in
                       case /=#
                              (word2Int#
                                 (and#
                                    (int2Word# ww_s5OwO)
                                    (xor# (int2Word# (negateInt# (word2Int# m_s5GU1))) m_s5GU1)))
                              dt_d5E7n
                       of {
                         __DEFAULT ->
                           case and# (int2Word# ww_s5OwO) m_s5GU1 of {
                             __DEFAULT -> jump go3_s5GTZ r_a5sqP;
                             0## -> jump go3_s5GTZ l_a5sqO
                           };
                         1# -> Nothing
                       };
                     Tip dt_d5E7p x_a5sqR -> jump exit_X9 dt_d5E7p x_a5sqR;
                     Nil -> Nothing
                   }; } in
             jump go3_s5GTZ w_s5OwL
       #+end_src
       Notice all those ~word2Int~ and ~int2Word~'s? The hypothesis here is that
      these are allocating. Even if they aren't they waste time in the
      conversion. You can see it more clearly in the stg:
       #+begin_src haskell
       $wlookup =
           \r [ww_s5Wim w_s5Win]
               let-no-escape {
                 exit_s5Wio =
                     \r [dt_s5Wip x_s5Wiq]
                         case ==# [ww_s5Wim dt_s5Wip] of {
                           __DEFAULT -> Nothing [];
                           1# -> Just [x_s5Wiq];
                         };
               } in
                 let-no-escape {
                   Rec {
                   go3_s5Wis =
                       \r [ds_s5Wit]
                           case ds_s5Wit of {
                             Bin dt_s5Wiv dt1_s5Wiw l_s5Wix r_s5Wiy ->
                                 case int2Word# [dt1_s5Wiw] of m_s5Wiz {
                                 __DEFAULT ->
                                 case word2Int# [m_s5Wiz] of sat_s5WiB {
                                 __DEFAULT ->
                                 case negateInt# [sat_s5WiB] of sat_s5WiC {
                                 __DEFAULT ->
                                 case int2Word# [sat_s5WiC] of sat_s5WiD {
                                 __DEFAULT ->
                                 case xor# [sat_s5WiD m_s5Wiz] of sat_s5WiE {
                                 __DEFAULT ->
                                 case int2Word# [ww_s5Wim] of sat_s5WiA {
                                 __DEFAULT ->
                                 case and# [sat_s5WiA sat_s5WiE] of sat_s5WiF {
                                 __DEFAULT ->
                                 case word2Int# [sat_s5WiF] of sat_s5WiG {
                                 __DEFAULT ->
                                 case /=# [sat_s5WiG dt_s5Wiv] of {
                                   __DEFAULT ->
                                       case int2Word# [ww_s5Wim] of sat_s5WiI {
                                       __DEFAULT ->
                                       case and# [sat_s5WiI m_s5Wiz] of {
                                         __DEFAULT -> go3_s5Wis r_s5Wiy;
                                         0## -> go3_s5Wis l_s5Wix;
                                       };
                                       };
                                   1# -> Nothing [];
                                 };
                                 };
                                 };
                                 };
                                 };
                                 };
                                 };
                                 };
                                 };
                             Tip dt_s5WiK x_s5WiL -> exit_s5Wio dt_s5WiK x_s5WiL;
                             Nil -> Nothing [];
                           };
                   end Rec }
                 } in  go3_s5Wis w_s5Win;
       #+end_src
       In the stg there are a lot of temporary fully evaluated variables like
      ~sat_s5WiB~ which is just the result of ~word2Int~ applied to the result
      of ~int2Word~ on variable ~m~, clearly what a waste!

      We can see why in the source code for ~lookup~ in ~IntMap~:
      #+begin_src haskell
      lookup :: Key -> IntMap a -> Maybe a
      lookup !k = go
        where
          go (Bin p m l r) | nomatch k p m = Nothing
                           | zero k m  = go l
                           | otherwise = go r
          go (Tip kx x) | k == kx   = Just x
                        | otherwise = Nothing
          go Nil = Nothing
      #+end_src
      Nothing too unusual but if we look at those helper functions we'll find a
      bunch of superfluous ~int2Word~ calls:
      #+begin_src haskell
      -- | Should this key follow the left subtree of a 'Bin' with switching
      -- bit @m@? N.B., the answer is only valid when @match i p m@ is true.
      zero :: Key -> Mask -> Bool
      zero i m
        = (natFromInt i) .&. (natFromInt m) == 0
      {-# INLINE zero #-}

      nomatch,match :: Key -> Prefix -> Mask -> Bool

      -- | Does the key @i@ differ from the prefix @p@ before getting to
      -- the switching bit @m@?
      nomatch i p m
        = (mask i m) /= p
      {-# INLINE nomatch #-}

      -- | Does the key @i@ match the prefix @p@ (up to but not including
      -- bit @m@)?
      match i p m
        = (mask i m) == p
      {-# INLINE match #-}


      -- | The prefix of key @i@ up to (but not including) the switching
      -- bit @m@.
      mask :: Key -> Mask -> Prefix
      mask i m
        = maskW (natFromInt i) (natFromInt m)
      {-# INLINE mask #-}


      {--------------------------------------------------------------------
        Big endian operations
      --------------------------------------------------------------------}

      -- | The prefix of key @i@ up to (but not including) the switching
      -- bit @m@.
      maskW :: Nat -> Nat -> Prefix
      maskW i m
        = intFromNat (i .&. ((-m) `xor` m))
      {-# INLINE maskW #-}
      #+end_src

      and that's where these superfluous calls are coming from. There is an
      extra call I want to point out which arises from ~-m~ in ~maskW~. If you
      check the ~Num~ instance for ~Word~ you'll see this:

      #+begin_src haskell
      instance Num Word64 where
          ...
          negate (W64# x#)       = W64# (int64ToWord64# (negateInt64# (word64ToInt64# x#)))
          ...
      #+end_src

      Which also does conversion! Why this is the case and not something like
      ~maxBound - x~ or even a call to a primop like ~0 - x~ I don't know.

      So I tried to fix it with this version of [[https://github.com/doyougnu/containers/commits/wip/intmap-less-alloc][lookup]]:

      #+begin_src Haskell
      lookup :: Key -> IntMap a -> Maybe a
      lookup !k = go
        where
          go (Bin p m l r)  | nomatchNat k' p' m' = Nothing
                            | zeroNat    k' m'    = go l
                            | otherwise           = go r
            where p' = natFromInt p
                  m' = natFromInt m
                  k' = natFromInt k
          go (Tip kx x) | k == kx   = Just x
                           | otherwise = Nothing
          go Nil = Nothing
      #+end_src
      Which just converts these Bin parameters /once/ and then uses Nat
      versions to do the Bit manipulation. If we look at the core and stg the
      situation looks much improved:
      #+begin_src haskell
      $wlookup
        = \ @a_s5MgS ww_s5MgX w_s5MgU ->
            let { k'_s5ES7 = int2Word# ww_s5MgX } in
            join {
              exit_X9 dt_d5BQu x_a5q9D
                = case ==# ww_s5MgX dt_d5BQu of {
                    __DEFAULT -> Nothing;
                    1# -> Just x_a5q9D
                  } } in
            joinrec {
              go3_s5ECW ds_d5wR4
                = case ds_d5wR4 of {
                    Bin dt_d5BQs dt1_d5BQt l_a5q9x r_a5q9y ->
                      let { m'_s5ECZ = int2Word# dt1_d5BQt } in
                      case neWord#
                             (and# k'_s5ES7 (xor# (minusWord# 0## m'_s5ECZ) m'_s5ECZ))
                             (int2Word# dt_d5BQs)
                      of {
                        __DEFAULT ->
                          case and# k'_s5ES7 m'_s5ECZ of {
                            __DEFAULT -> jump go3_s5ECW r_a5q9y;
                            0## -> jump go3_s5ECW l_a5q9x
                          };
                        1# -> Nothing
                      };
                    Tip dt_d5BQu x_a5q9D -> jump exit_X9 dt_d5BQu x_a5q9D;
                    Nil -> Nothing
                  }; } in
            jump go3_s5ECW w_s5MgU
      #+end_src

      That's 3 ~int2Word~'s instead of 4, and no calls to ~word2Int~! This is
      even more clear in the ~stg~:

      #+begin_src haskell
      $wlookup =
          \r [ww_s5TXH w_s5TXI]
              case int2Word# [ww_s5TXH] of k'_s5TXJ {
              __DEFAULT ->
              let-no-escape {
                exit_s5TXK =
                    \r [dt_s5TXL x_s5TXM]
                        case ==# [ww_s5TXH dt_s5TXL] of {
                          __DEFAULT -> Nothing [];
                          1# -> Just [x_s5TXM];
                        };
              } in
                let-no-escape {
                  Rec {
                  go3_s5TXO =
                      \r [ds_s5TXP]
                          case ds_s5TXP of {
                            Bin dt_s5TXR dt1_s5TXS l_s5TXT r_s5TXU ->
                                case int2Word# [dt1_s5TXS] of m'_s5TXV {
                                __DEFAULT ->
                                case int2Word# [dt_s5TXR] of sat_s5TXZ {
                                __DEFAULT ->
                                case minusWord# [0## m'_s5TXV] of sat_s5TXW {
                                __DEFAULT ->
                                case xor# [sat_s5TXW m'_s5TXV] of sat_s5TXX {
                                __DEFAULT ->
                                case and# [k'_s5TXJ sat_s5TXX] of sat_s5TXY {
                                __DEFAULT ->
                                case neWord# [sat_s5TXY sat_s5TXZ] of {
                                  __DEFAULT ->
                                      case and# [k'_s5TXJ m'_s5TXV] of {
                                        __DEFAULT -> go3_s5TXO r_s5TXU;
                                        0## -> go3_s5TXO l_s5TXT;
                                      };
                                  1# -> Nothing [];
                                };
                                };
                                };
                                };
                                };
                                };
                            Tip dt_s5TY2 x_s5TY3 -> exit_s5TXK dt_s5TY2 x_s5TY3;
                            Nil -> Nothing [];
                          };
                  end Rec }
                } in  go3_s5TXO w_s5TXI;
              };
      #+end_src

      In the stg we see a reduction in ~case~ expressions from 11 to 7! However,
      the change doesn't show up in /any/ benchmarking as a positive. IntMap
      benchmarks are unchanged, allocations of ~lookup~ are unchanged in a ticky
      of ~spectral/simple/Main.hs~ with a patched ~GHC~. When compiling packages
      with the patched GHC allocations were actually found to /get worse/! The
      reason is in the ~Cmm~ code. Essentially the patched version produces
      better ~stg~ but gets optimized away at ~Cmm~ anyway. Furthermore
      because we allocate for ~k~ in the closure of the patched version the
      patched ~Cmm~ code maintains an additional register, whereas the
      un-patched version doesn't. Thus we have another promising lead but a
      failure in the end.

      Addendum (7-22-2021). These coercions get compiled away in the `Stg to Cmm` pass. Specifically [here](https://gitlab.haskell.org/ghc/ghc/-/blob/master/compiler/GHC/StgToCmm/Prim.hs#L1089)

    - Given the results of the ~weigh~ benchmarking above I altered
      ~lookupBndrSwap~ to avoid ~lookupUFM~. Here's the master version:
      #+begin_src haskell
        lookupBndrSwap :: OccEnv -> Id -> (CoreExpr, Id)
        -- See Note [The binder-swap substitution]
        -- Returns an expression of the same type as Id
        lookupBndrSwap env@(OccEnv { occ_bs_env = bs_env })  bndr
          = case lookupVarEnv bs_env bndr of {
               Nothing           -> (Var bndr, bndr) ;
               Just (bndr1, mco) ->

            -- Why do we iterate here?
            -- See (BS2) in Note [The binder-swap substitution]
            case lookupBndrSwap env bndr1 of
              (fun, fun_id) -> (add_cast fun mco, fun_id) }

          where
            add_cast fun MRefl    = fun
            add_cast fun (MCo co) = Cast fun (mkSymCo co)
            -- We must switch that 'co' to 'sym co';
            -- see the comment with occ_bs_env
            -- No need to test for isReflCo, because 'co' came from
            -- a (Cast e co) and hence is unlikely to be Refl
      #+end_src

      and the change:

      #+begin_src haskell
        lookupBndrSwap :: OccEnv -> Id -> (CoreExpr, Id)
        -- See Note [The binder-swap substitution]
        -- Returns an expression of the same type as Id
        lookupBndrSwap env@(OccEnv { occ_bs_env = bs_env })  bndr
          = if bndr `elemVarEnv` bs_env
            then -- Why do we iterate here?
                 -- See (BS2) in Note [The binder-swap substitution]
                 let (bndr1, mco) = lookupVarEnv_NF bs_env bndr
                     in case lookupBndrSwap env bndr1 of
                        (fun, fun_id) -> (add_cast fun mco, fun_id)
            else (Var bndr, bndr)

          where
            add_cast fun MRefl    = fun
            add_cast fun (MCo co) = Cast fun (mkSymCo co)
            -- We must switch that 'co' to 'sym co';
            -- see the comment with occ_bs_env
            -- No need to test for isReflCo, because 'co' came from
            -- a (Cast e co) and hence is unlikely to be Refl
      #+end_src
      Notice that the case statement which unboxes the ~Maybe~ is removed and
      replaced with an ~if-expression~.

      This change results in a 70% percent reduction in allocations from a ticky
      of ~spectral/simple/Main.hs~. Here's the master ticky filtered for
      ~lookup~:
      #+begin_src shell
    527596   59018048          0   8 SiMMMMSS             $s$wlookupBndrSwap{v repE} (GHC.Core.Opt.OccurAnal) (fun)
    4233024   55521472          0   2 iM                   Data.IntMap.Internal.$wlookup{v rg3a} (fun)
      95963   11782944          0   6 ST>MLL               GHC.Core.Rules.lookupRule{v r6NZ} (fun)
      28446    2283960          0   2 >M                   $llookupAddr_g8tO{v} (GHC.CmmToAsm.X86.Instr) (fun)
       8046    2252880          0   7 SMiipSM              GHC.Unit.State.$wlookupModuleWithSuggestions'{v rkVG} (fun)
      70028    2157088          0   3 SSM                  GHC.Core.Subst.$w$slookupIdSubst{v r8Vc} (fun)
      #+end_src

      Note that ~lookupBndrSwap~ allocations more (2nd column) than ~lookup~.
      Here's the patch ticky:
       #+begin_src shell
     3699354   55354240          0   2 iM                   Data.IntMap.Internal.$wlookup{v rg3a} (fun)
     529222   16961120          0   2 SM                   $wlookupBndrSwap{v repA} (GHC.Core.Opt.OccurAnal) (fun)
      95963   11782944          0   6 ST>MLL               GHC.Core.Rules.lookupRule{v r8} (fun)
      28446    2283960          0   2 >M                   $llookupAddr_g8tO{v} (GHC.CmmToAsm.X86.Instr) (fun)
       8046    2252880          0   7 SMiipSM              GHC.Unit.State.$wlookupModuleWithSuggestions'{v rkVG} (fun)
      70028    2157088          0   3 SSM                  GHC.Core.Subst.$w$slookupIdSubst{v r1l} (fun)
       8030    1862960          0   8 MSMEiipS             GHC.Unit.State.$wlookupModuleInAllUnits{v rkVy} (fun)
       5758     598832          0   3 SSL                  $wlookupOccs{v rkpI} (GHC.Core.Opt.SpecConstr) (fun)
      17952     334720          0   2 SM                   $wlookupHowBound{v rkpD} (GHC.Core.Opt.SpecConstr) (fun)
       #+end_src
       We see a change in allocations for ~lookupBndrSwap~ from 59018048 to 16961120 (a 71% reduction).



** Avoid allocations in substitutions in the simplifier

*** Hypothesis
    Benchmarking indicates that a large amount of allocations occur in the
    simplifier. We should seek to understand why that is the case.

*** Status
    Unexplored

*** Evidence

*** The Fix

*** Relevant Issues
    - [[https://gitlab.haskell.org/ghc/ghc/-/issues/19537][Opportunity for increased sharing during substitution]]
    - [[https://gitlab.haskell.org/ghc/ghc/-/issues/19538][Annotating Core to avoid unnecessary traversal of large subexpressions]]

*** Relevant Merge Requests
    - Sylvain Henry implemented a fix only in ~Tidy~ in [[https://gitlab.haskell.org/ghc/ghc/-/merge_requests/5267][!5267]] but there is a bug
      and some variables aren't correctly renamed leading to test failures.

*** Relevant Patches

*** Courses of Action
    1. Read through [[https://gitlab.haskell.org/ghc/ghc/-/merge_requests/5267][!5267]]
    2. Fix [[https://gitlab.haskell.org/ghc/ghc/-/merge_requests/5267][!5267]] benchmark it. Try it out in ~GHC.Core.substExpr~ and
       ~GHC.Core.TyCo.Subst~

** Optimize the pretty printing during code generation

*** Hypothesis
    Code generation is a significant chunk of compile time. According to Matt
    Pickering some pretty printing functions perform a lot of allocation during
    this phase which leads to a slow down.

*** Status
    Issue Observed

*** Evidence
    Consider this ticky profile sorted by allocations (second column) from ~spectral/simple/Main.hs~:
    #+begin_src shell
     16345250 1954658904          0   4 MEiM                 $waboveNest{v r4VL} (GHC.Utils.Ppr) (fun)
      12426097 1127986424          0   3 MEM                  beside{v rTN} (GHC.Utils.Ppr) (fun)
      11513051  695793312          0   3 i.M                  Data.IntMap.Internal.$winsert{v rg4h} (fun)
       1168605  281826888          0   5 SMMSM                GHC.Core.Opt.Simplify.simplExpr2{v roRP} (fun)
       3034078  233802360          0   2 >L                   GHC.Base.map{v 01X} (fun)
        531047  152702648          0   4 SMLL                 $woccAnalApp{v raIX} (GHC.Core.Opt.OccurAnal) (fun)
        486824  147994496          0   2 SS                   GHC.IO.Encoding.UTF8.mkUTF1{v r2sh} (fun)
       1011674  119141568          0   5 SSMSM                rebuildCall{v roV4} (GHC.Core.Opt.Simplify) (fun)
        352269  109907928   11272608   7 >.pMpME              $wact1{v s5qY} (GHC.IO.Handle.Text) (fun) in r5im
       4977172  104923112          0   2 iM                   Data.IntMap.Internal.$wdelete{v rg4i} (fun)
        858053   98320600          0   4 iiiM                 $l$s$wget1_g5mo{v} (GHC.Utils.Ppr) (fun)
        497211   98275320          0   5 SMLiM                GHC.Core.Opt.Simplify.Utils.$wmkArgInfo{v rjZu} (fun)
    #+end_src
    The pretty printer defined in ~Ppr~ does /more/ allocations than ~insert~!
    Clearly that should not be the case. Furthermore we have evidence from a
    heap profile that these are a result of a memory leak:
    #+begin_src
        Thu Jul 15 18:42 2021 Time and Allocation Profiling Report  (Final)

           ghc +RTS -p -s -hy -l-au -rsimple.ticky -RTS Main.hs -fforce-recomp -O2 -ticky-LNE -ticky-allocd -ddump-stg-final -ddump-simpl -ddump-to-file

        total time  =        7.36 secs   (7356 ticks @ 1000 us, 1 processor)
        total alloc = 8,120,252,504 bytes  (excludes profiling overheads)

        COST CENTRE                      MODULE                 SRC                                                  %time %alloc

        simplIdF                         GHC.Core.Opt.Simplify  compiler/GHC/Core/Opt/Simplify.hs:1122:61-79          24.4   21.4
        doCodeGen                        GHC.Driver.Main        compiler/GHC/Driver/Main.hs:(1766,1)-(1814,46)        13.7   23.5
        CoreTidy                         GHC.Driver.Main        compiler/GHC/Driver/Main.hs:896:15-58                  8.3   13.4
        occAnalBind.assoc                GHC.Core.Opt.OccurAnal compiler/GHC/Core/Opt/OccurAnal.hs:809:13-64           6.8    5.7
        OccAnal                          GHC.Core.Opt.Pipeline  compiler/GHC/Core/Opt/Pipeline.hs:(713,22)-(714,42)    5.8    5.0
        simplRecOrTopPair-normal         GHC.Core.Opt.Simplify  compiler/GHC/Core/Opt/Simplify.hs:(307,5)-(308,62)     2.6    1.9
        FloatOutwards                    GHC.Core.Opt.Pipeline  compiler/GHC/Core/Opt/Pipeline.hs:499:34-82            2.5    2.0
        pprNativeCode                    GHC.CmmToAsm           compiler/GHC/CmmToAsm.hs:427:37-64                     1.8    1.7
        StgToCmm                         GHC.Driver.Main        compiler/GHC/Driver/Main.hs:1785:13-97                 1.8    1.3
        simplNonRecE                     GHC.Core.Opt.Simplify  compiler/GHC/Core/Opt/Simplify.hs:1198:31-78           1.6    1.1
        rebuild                          GHC.Core.Opt.Simplify  compiler/GHC/Core/Opt/Simplify.hs:1123:60-85           1.6    1.1
        tc_rn_src_decls                  GHC.Tc.Module          compiler/GHC/Tc/Module.hs:(592,4)-(663,7)              1.4    0.8
        RegAlloc-linear                  GHC.CmmToAsm           compiler/GHC/CmmToAsm.hs:(586,27)-(588,55)             1.4    1.0
        GHC.CmmToAsm.CFG.mkGlobalWeights GHC.CmmToAsm.CFG       compiler/GHC/CmmToAsm/CFG.hs:954:1-15                  1.4    1.5
        DmdAnal                          GHC.Core.Opt.Pipeline  compiler/GHC/Core/Opt/Pipeline.hs:511:34-103           1.3    1.4
        simplExprF1-Lam                  GHC.Core.Opt.Simplify  compiler/GHC/Core/Opt/Simplify.hs:1160:5-39            1.3    0.8
        sink                             GHC.Cmm.Pipeline       compiler/GHC/Cmm/Pipeline.hs:(115,12)-(116,58)         1.2    1.0
        regLiveness                      GHC.CmmToAsm           compiler/GHC/CmmToAsm.hs:(504,17)-(505,75)             1.2    1.1
    #+end_src
    You can see that ~doCodeGen~ takes 13% of time but a whopping 23.5% of
    allocations. If we peak that the source we'll find stuff like this:
    #+begin_src haskell
    putDumpFileMaybe logger Opt_D_dump_stg_final "Final STG:" FormatSTG (pprGenStgTopBindings (initStgPprOpts dflags) stg_binds_w_fvs)
    #+end_src
    where ~pprGenStgBinding~ is:
    #+begin_src haskell
      pprGenStgTopBindings :: (OutputablePass pass) => StgPprOpts -> [GenStgTopBinding pass] -> SDoc
      pprGenStgTopBindings opts binds = vcat $ intersperse blankLine (map (pprGenStgTopBinding opts) binds)
    #+end_src
    and ~vcat~ is a /lazy/ fold!:
    #+begin_src haskell
      -- | List version of '$$'.
      vcat :: [Doc] -> Doc
      vcat = reduceAB . foldr (above_' False) empty
    #+end_src

    However this is not definitive proof, we would need to observe the core and
    stg to really verify these thunks.

*** The Fix
    - Note taken on [2021-07-25 Sun 17:09] \\
      Adding some strictness produced the following ticky profile:
      #+begin_src  shell
       17371793 1949818584          0   4 iEMM                 $saboveNest{v r4Uk} (GHC.Utils.Ppr) (fun)
         13224056 1116014480          0   3 MEM                  beside{v r14Y} (GHC.Utils.Ppr) (fun)
         11442227  691716208          0   3 i.M                  Data.IntMap.Internal.$winsert{v rg6b} (fun)
          1163713  280712792          0   5 SMMSM                GHC.Core.Opt.Simplify.simplExpr2{v roJf} (fun)
      #+end_src
      Still need to check time with nofib

    - Note taken on [2021-07-25 Sun 16:43] \\
      Update. I built a stage 2 profiled compiler with the previously mentioned patch
      but the build went into an infinite loop. I've narrowed down the cause to adding
      strictness to the ~Doc~ data type. Specifically this builds just fine:
      #+begin_src haskell
      -- | The abstract type of documents.
      -- A Doc represents a *set* of layouts. A Doc with
      -- no occurrences of Union or NoDoc represents just one layout.
      data Doc
        = Empty                                            -- empty
        | NilAbove Doc                                     -- text "" $$ x
        | TextBeside !TextDetails {-# UNPACK #-} !Int Doc  -- text s <> x
        | Nest {-# UNPACK #-} !Int Doc                     -- nest k x
        | Union !Doc !Doc                                    -- ul `union` ur
        | NoDoc                                            -- The empty set of documents
        | Beside !Doc Bool !Doc                              -- True <=> space between
        | Above !Doc Bool !Doc                               -- True <=> never overlap
      #+end_src
      while this goes infinite:
      #+begin_src
      data Doc
        = Empty                                            -- empty
        | NilAbove !Doc                                     -- text "" $$ x
        | TextBeside !TextDetails {-# UNPACK #-} !Int !Doc  -- text s <> x
        | Nest {-# UNPACK #-} !Int !Doc                     -- nest k x
        | Union !Doc !Doc                                    -- ul `union` ur
        | NoDoc                                            -- The empty set of documents
        | Beside !Doc Bool !Doc                              -- True <=> space between
        | Above !Doc Bool !Doc                               -- True <=> never overlap
      #+end_src
      Notice the extra bangs on ~NilAbove~ ~TextBeside~ and ~Nest~.

    I've forked and pushed a patch [[https://gitlab.haskell.org/doyougnu/ghc/-/commit/659db2e3a75c585b3a50b25b8b2f84aa512850d1][here]] it cleans up that lazy fold and removes lines like this:
    #+begin_src
    put b _ | b `seq` False = undefined
    #+end_src
    Whose only purpose is to make ~b~ strict. So I've removed those lines for
    bang patterns:
    #+begin_src haskell
    put !b (Chr c)    = bPutChar b c
    put !b (Str s)    = bPutStr  b s
    put !b (PStr s)   = bPutFS   b s
    #+end_src
    I suspect this module has not been updated in quite a while because of old
    tricks like that in the code.

*** Relevant Issues
    - there is an old issue on ~pretty~ by Ben [[https://github.com/haskell/pretty/issues/44][here]] which highlights the a
      specific use case for GHC. The same functions that Ben points to are the
      same ones that sit at the top of my ticky profile above.
    - Most of the issues on ~pretty~ are relevant. Because ~pretty~ uses
      String's there is a bunch of problems with quadratic runtimes and space
      leaks. It's simply the wrong data structure. I think the right thing to do
      is use a different library like [[https://hackage.haskell.org/package/prettyprinter][this]] one, but that would mean adding a
      dependency on ~text~. I'm unsure if this would mean adding ~text~ to base
      or simply using ~text~ as a boot library.

*** Relevant Merge Requests

*** Relevant Patches

*** Courses of Action
    1. benchmark pretty printing during code generation to identify candidate
       functions for optimization.
    2. Ticky profile these functions to get some hard evidence.


* Knowledge Sharing
  It would be nice to know:

** Is every IntMap necessary?
   - Consider this passage from Richard Eisenberg, in ghc-devs Vol215 issue 5:
     #+begin_quote
     One piece I'm curious about, reading this thread: why do we have so many IntMaps
     and operations on them? Name lookup is a fundamental operation a compiler must
     do, and that would use an IntMap: good. But maybe there are other IntMaps used
     that are less necessary. A key example: whenever we do substitution, we track an
     InScopeSet, which is really just an IntMap. This InScopeSet remembers the name
     of all variables in scope, useful when we need to create a new variable name
     (this is done by uniqAway). Yet perhaps the tracking of these in-scope variables
     is very expensive and comprises much of the IntMap time. Might it be better just
     to always work in a monad capable of giving fresh names? We actually don't even
     need a monad, if that's too annoying. Instead, we could just pass around an
     infinite list of fresh uniques. This would still be clutterful, but if it grants
     us a big speed improvement, the clutter might be worth it.

     The high-level piece here is that there may be good things that come from
     understanding where these IntMaps arise.
     #+end_quote